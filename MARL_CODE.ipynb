{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMQxDugMGf5QtyfwnLq7IdP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlirezaRamezaney/MARL-Cournot-game/blob/main/MARL_CODE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "gF3V5YlMPb4A"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "from gym import spaces\n",
        "\n",
        "class CournotCompetitionEnv(gym.Env):\n",
        "    def init(self, agent_id, n_agents=4):\n",
        "        super(CournotCompetitionEnv, self).__init__()\n",
        "        self.n_agents = n_agents\n",
        "        self.agent_id = agent_id\n",
        "        self.action_space = spaces.Box(low=0, high=1, shape=(1,), dtype=np.float32)\n",
        "        self.observation_space = spaces.Box(low=0, high=1, shape=(2,), dtype=np.float32)\n",
        "        self.last_total_output = 0.5\n",
        "        self.last_reward = 0.0\n",
        "\n",
        "    def step(self, action):\n",
        "        self.actions[self.agent_id] = float(np.clip(action, 0, 1))\n",
        "        if all(self.actions[i] is not None for i in range(self.n_agents)):\n",
        "            total_output = sum(self.actions)\n",
        "            price = max(1 - total_output, 0)\n",
        "            rewards = [q * price - 0.1 * q for q in self.actions]\n",
        "            obs = np.array([total_output, rewards[self.agent_id]], dtype=np.float32)\n",
        "            reward = rewards[self.agent_id]\n",
        "            self.last_total_output = total_output\n",
        "            self.last_reward = reward\n",
        "            done = True\n",
        "            self.actions = [None] * self.n_agents\n",
        "            return obs, reward, done, {}\n",
        "        else:\n",
        "            return np.array([self.last_total_output, self.last_reward], dtype=np.float32), 0.0, False, {}\n",
        "\n",
        "    def reset(self):\n",
        "        self.last_total_output = 0.5\n",
        "        self.last_reward = 0.0\n",
        "        self.actions = [None] * self.n_agents\n",
        "        return np.array([self.last_total_output, self.last_reward], dtype=np.float32)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from stable_baselines3 import PPO, DDPG\n",
        "from stable_baselines3.common.env_util import DummyVecEnv\n",
        "\n",
        "n_agents = 4\n",
        "models = []\n",
        "\n",
        "# Agent 0 and 1 use PPO, 2 and 3 use DDPG\n",
        "for i in range(n_agents):\n",
        "    env = DummyVecEnv([lambda i=i: CournotCompetitionEnv(agent_id=i)])\n",
        "    model = PPO(\"MlpPolicy\", env, verbose=0) if i < 2 else DDPG(\"MlpPolicy\", env, verbose=0)\n",
        "    model.learn(total_timesteps=100_000)\n",
        "    models.append(model)"
      ],
      "metadata": {
        "id": "0nYMHnF5PhlC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "n_agents = 4\n",
        "actions_history = [[] for _ in range(n_agents)]\n",
        "rewards_history = [[] for _ in range(n_agents)]\n",
        "\n",
        "envs = [CournotCompetitionEnv(agent_id=i) for i in range(n_agents)]\n",
        "obs = [env.reset() for env in envs]\n",
        "\n",
        "for _ in range(500):\n",
        "    actions = [float(models[i].predict(obs[i], deterministic=True)[0]) for i in range(n_agents)]\n",
        "    results = [envs[i].step(actions[i]) for i in range(n_agents)]\n",
        "    obs = [r[0] for r in results]\n",
        "    for i in range(n_agents):\n",
        "        actions_history[i].append(actions[i])\n",
        "        rewards_history[i].append(results[i][1])\n",
        "\n",
        "plt.figure(figsize=(12,5))\n",
        "for i in range(n_agents):\n",
        "    plt.plot(actions_history[i], label=f\"Agent {i} ({'PPO' if i<2 else 'DDPG'})\")\n",
        "plt.axhline(0.2, color='black', linestyle='--', label='Nash q=0.2')\n",
        "plt.title(\"Agent Actions Over Time\")\n",
        "plt.xlabel(\"Episode\")\n",
        "plt.ylabel(\"Quantity (Action)\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rv4jScDNPhEw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}